{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/algu/anaconda3/envs/keras/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.models import load_model\n",
    "from keras.utils import Sequence\n",
    "import keras.backend as K\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from ast import literal_eval\n",
    "import time\n",
    "\n",
    "from TextAnalisys.wordListToVecListConverter import WordListToVecListConverter\n",
    "from TextAnalisys.wordToVecConverter import WordToVecConverterOneHotEncoder\n",
    "from modelTrainer import ModelTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convertToVectorizedData(frame):\n",
    "    converter = WordListToVecListConverter()\n",
    "    wordLists = frame[\"text\"].apply(literal_eval).values\n",
    "    wordConverter = WordToVecConverterOneHotEncoder()\n",
    "    wordConverter.fit(wordLists)\n",
    "    converter.setWordToVecConverter(wordConverter)\n",
    "    vectorizedWordLists = []\n",
    "    for wordList in wordLists:\n",
    "        vectorizedWordLists.append(converter.convert(wordList))\n",
    "    vectorizedDataFrame = pd.DataFrame()\n",
    "    vectorizedDataFrame[\"vector\"] = vectorizedWordLists\n",
    "    vectorizedDataFrame[\"label\"] = frame[\"label\"]\n",
    "    return vectorizedDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cleanDocumentRepresentationsVectors(documentRepresentations):\n",
    "    emptyDocumentIndexes = []\n",
    "    for idx, row in documentRepresentations.T.iteritems():\n",
    "        if row[\"vector\"] == []:\n",
    "            emptyDocumentIndexes.append(idx)\n",
    "    documentRepresentations.drop(emptyDocumentIndexes, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def padSequence(sequence, necessaryNumberOfWordsInDocument):\n",
    "    if len(sequence) == 0:\n",
    "        return None\n",
    "    wordVectorLength = None\n",
    "    res = None\n",
    "    if len(sequence[0].shape) == 2:\n",
    "        wordVectorLength = sequence[0].shape[-1]\n",
    "        res = np.zeros(shape=(len(sequence), necessaryNumberOfWordsInDocument, wordVectorLength), dtype=np.int32)\n",
    "    else:\n",
    "        wordVectorLength = 1\n",
    "        res = np.zeros(shape=(len(sequence), necessaryNumberOfWordsInDocument))\n",
    "    i = 0\n",
    "    while i < len(sequence):\n",
    "        realNumberOfWordsInDocument = sequence[i].shape[0]\n",
    "        difference = necessaryNumberOfWordsInDocument - realNumberOfWordsInDocument\n",
    "        if difference > 0:\n",
    "            res[i] = np.insert(sequence[i], [0] * difference, 0, axis=0)\n",
    "        else:\n",
    "            difference *= -1\n",
    "            res[i] = np.delete(sequence[i], [-1] * difference, axis=0)\n",
    "        i += 1\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def GeneratorInfinite(X, y, batch_size, necessaryNumberOfWordsInDocument):\n",
    "    modulo = len(X) % batch_size\n",
    "    maxIndex = None\n",
    "    numberOfDataPieces = None\n",
    "    if modulo != 0:\n",
    "        numberOfDataPieces = math.ceil(len(X) / batch_size)\n",
    "    else:\n",
    "        numberOfDataPieces = math.floor(len(X) / batch_size)\n",
    "    idx = 0\n",
    "    while True:\n",
    "        X_res = None\n",
    "        y_res = None\n",
    "        beginIndexInSequence = idx * batch_size\n",
    "        sequenceLength = None\n",
    "        if idx < numberOfDataPieces - 1:\n",
    "            sequenceLength = batch_size\n",
    "        else:\n",
    "            sequenceLength = len(X) - beginIndexInSequence\n",
    "        X_res = pad_sequences(X[beginIndexInSequence : beginIndexInSequence + sequenceLength], \n",
    "                             necessaryNumberOfWordsInDocument, \n",
    "                             truncating='post')\n",
    "        '''X_res = padSequence(X[beginIndexInSequence : beginIndexInSequence + sequenceLength], \n",
    "                             necessaryNumberOfWordsInDocument)'''\n",
    "        y_res = y[beginIndexInSequence : beginIndexInSequence + sequenceLength]\n",
    "        yield (X_res, y_res)\n",
    "        idx += 1\n",
    "        idx = idx % numberOfDataPieces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GeneratorFinite(Sequence):\n",
    "    def __init__(self, X, y, batchSize, necessaryNumberOfWordsInDocument):\n",
    "        self.__X = X\n",
    "        self.__y = y\n",
    "        self.__batchSize = batchSize\n",
    "        self.__necessaryNumberOfWordsInDocument = necessaryNumberOfWordsInDocument\n",
    "        \n",
    "        notDivisibleByBatchSize = len(self.__X) % self.__batchSize\n",
    "        if notDivisibleByBatchSize != 0:\n",
    "            self.__len = math.ceil(len(self.__X) / self.__batchSize)\n",
    "        else:\n",
    "            self.__len = math.floor(len(self.__X) / self.__batchSize)\n",
    "    def __len__(self):\n",
    "        return self.__len\n",
    "    def __getitem__(self, idx):\n",
    "        beginIndexInSequence = idx * self.__batchSize\n",
    "        numberOfEntities = None\n",
    "        if idx < self.__len__() - 1:\n",
    "            numberOfEntities = self.__batchSize\n",
    "        else:\n",
    "            numberOfEntities = len(self.__X) - beginIndexInSequence\n",
    "        X_res = pad_sequences(self.__X[beginIndexInSequence : beginIndexInSequence + numberOfEntities], \n",
    "                             self.__necessaryNumberOfWordsInDocument, \n",
    "                             truncating='post')\n",
    "        '''X_res = padSequence(self.__X[beginIndexInSequence : beginIndexInSequence + numberOfEntities], \n",
    "                             self.__necessaryNumberOfWordsInDocument)'''\n",
    "        y_res = self.__y[beginIndexInSequence : beginIndexInSequence + numberOfEntities]\n",
    "        return (X_res, y_res)\n",
    "    __X = None\n",
    "    __y = None\n",
    "    __batchSize = None\n",
    "    __necessaryNumberOfWordsInDocument = None\n",
    "    __len = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_validate_test_split(df, train_percent=0.7, validate_percent=0.1, seed=None):\n",
    "    np.random.seed(seed)\n",
    "    perm = np.random.permutation(df.index)\n",
    "    m = len(df)\n",
    "    train_end = int(train_percent * m)\n",
    "    validate_end = int(validate_percent * m) + train_end\n",
    "    train = df.ix[perm[:train_end]]\n",
    "    validate = df.ix[perm[train_end:validate_end]]\n",
    "    test = df.ix[perm[validate_end:]]\n",
    "    return train, validate, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/algu/anaconda3/envs/keras/lib/python3.5/site-packages/ipykernel_launcher.py:1: FutureWarning: from_csv is deprecated. Please use read_csv(...) instead. Note that some of the default arguments are different, so please refer to the documentation for from_csv when changing your function calls\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "frame = pd.DataFrame.from_csv(\"databasePositiveNegativeSeparatedWords.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/algu/anaconda3/envs/keras/lib/python3.5/site-packages/ipykernel_launcher.py:4: DeprecationWarning: elementwise == comparison failed; this will raise an error in the future.\n",
      "  after removing the cwd from sys.path.\n",
      "/home/algu/anaconda3/envs/keras/lib/python3.5/site-packages/ipykernel_launcher.py:4: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  after removing the cwd from sys.path.\n",
      "/home/algu/anaconda3/envs/keras/lib/python3.5/site-packages/ipykernel_launcher.py:7: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "data = convertToVectorizedData(frame)\n",
    "cleanDocumentRepresentationsVectors(data)\n",
    "train, validation, test = train_validate_test_split(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "constantNumberOfWordsPerDocument = 40\n",
    "batchSize = 5000\n",
    "max_features = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainGenerator = GeneratorInfinite(train[\"vector\"].values, \n",
    "                                   train[\"label\"].values, \n",
    "                                   batchSize, \n",
    "                                   constantNumberOfWordsPerDocument)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainFiniteGenerator = GeneratorFinite(train[\"vector\"].values, \n",
    "                                       train[\"label\"].values, \n",
    "                                       batchSize, \n",
    "                                       constantNumberOfWordsPerDocument)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testGenerator = GeneratorFinite(test[\"vector\"].values, \n",
    "                                test[\"label\"].values, \n",
    "                                batchSize, \n",
    "                                constantNumberOfWordsPerDocument)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "validationGenerator = GeneratorFinite(validation[\"vector\"].values, \n",
    "                                validation[\"label\"].values, \n",
    "                                batchSize, \n",
    "                                constantNumberOfWordsPerDocument)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128, input_length=constantNumberOfWordsPerDocument))\n",
    "model.add(LSTM(64, return_sequences=True))\n",
    "model.add(LSTM(64))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('tanh'))\n",
    "\n",
    "model.compile(loss='mean_absolute_error',\n",
    "              optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model = load_model(\"sentimentModel\")\n",
    "\n",
    "trainer = ModelTrainer(10, \"sentimentModel\")\n",
    "trainer.setModel(model)\n",
    "trainer.setGenerators(trainGenerator, testGenerator)\n",
    "trainer.setTrainFiniteGeneratorForTestModel(trainFiniteGenerator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of steps before testing step: 10\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 3s 3s/step - loss: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 612ms/step - loss: 0.9991\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 611ms/step - loss: 0.9982\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 611ms/step - loss: 0.9969\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 611ms/step - loss: 0.9955\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 612ms/step - loss: 0.9940\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 615ms/step - loss: 0.9925\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 654ms/step - loss: 0.9877\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 612ms/step - loss: 0.9848\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 613ms/step - loss: 0.9780\n",
      "Number of steps made: 10\n",
      "Current loss on train data: 0.9905693893864069\n",
      "Current loss: 0.9897886681148897\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 613ms/step - loss: 0.9752\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 617ms/step - loss: 0.9707\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 630ms/step - loss: 0.9642\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 640ms/step - loss: 0.9598\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 613ms/step - loss: 0.9515\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 612ms/step - loss: 0.9432\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 612ms/step - loss: 0.9303\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 613ms/step - loss: 0.9097\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 612ms/step - loss: 0.8922\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 613ms/step - loss: 0.8778\n",
      "Number of steps made: 20\n",
      "Current loss on train data: 0.9545933449214211\n",
      "Current loss: 0.9578045168853777\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-c6d332ea5ca6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshowModelOutputDuringTrainingSteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/projects/sentimentAnalysis/modelTrainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, showModelOutputDuringTrainingSteps)\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Current loss: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mminimumLoss\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mminimumLoss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstoreModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m                 \u001b[0mminimumLoss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstopTraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/projects/sentimentAnalysis/modelTrainer.py\u001b[0m in \u001b[0;36mstoreModel\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstoreModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__filenameToStoreBestModel\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__filenameToStoreBestModel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshowModelOutputDuringTrainingSteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__continueTraining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/keras/lib/python3.5/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, filepath, overwrite, include_optimizer)\u001b[0m\n\u001b[1;32m   2571\u001b[0m         \"\"\"\n\u001b[1;32m   2572\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msave_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2573\u001b[0;31m         \u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_optimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2574\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2575\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/keras/lib/python3.5/site-packages/keras/models.py\u001b[0m in \u001b[0;36msave_model\u001b[0;34m(model, filepath, overwrite, include_optimizer)\u001b[0m\n\u001b[1;32m    179\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m                             \u001b[0mparam_dset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/keras/lib/python3.5/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    315\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 317\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwith_phil\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/keras/lib/python3.5/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mclose\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    297\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mid_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfile_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m                     \u001b[0;32mwhile\u001b[0m \u001b[0mid_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m                         \u001b[0mh5i\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdec_ref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train(showModelOutputDuringTrainingSteps=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def custom_metric(y_true, y_pred):\n",
    "    return K.mean(K.less(K.abs(y_true - y_pred), 1), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='mean_absolute_error',\n",
    "              metrics=[custom_metric],\n",
    "              optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7990858593728677, 0.6028037350069718]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate_generator(testGenerator, use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = load_model(\"sentimentModel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras",
   "language": "python",
   "name": "keras"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
